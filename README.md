# Introduction to Explainable AI-Part1
## Introduction

The use of machine learning and AI has increased in recent years. However, continuing adoption is being hampered by some restrictions. One of the major problems with modern machine learning and deep learning algorithms is that their models are not easily interpretable or explainable. As algorithms improve in their predictive abilities, it becomes critical to understand the reasoning behind a given prediction.
A lack of interpretability and explainability in real AI applications makes it hard to trust their predictions. Providing human-friendly explanations would increase trust in machine learning systems and encourage their further use. Emerging as a critical area of study in the next years, explainable AI is now a hot topic in the AI community. This article is an introduction to a series of articles we will publish with an attempt to unbox the black-box model to increase the explainability of the decision made by AI algorithms.

## Mystery of the Black Box
As a result of recent innovations, machine learning algorithms have improved considerably in predictive ability and accuracy. On the other hand, their complexity has grown with time. It's possible to visualize the model's outputs for a given set of inputs without knowing how the model works. Machine learning models can learn the mapping of inputs to outputs by analyzing historical data. This mapping is obvious in certain models, such as decision trees. The prediction process becomes almost impossible for other models, such as random forests and deep learning. The inner workings and subtleties of many machine learning and deep learning models are often hidden.
The confidence we put in and use of these models might suffer as a result of our inability to fully grasp or explain them. How can we evaluate the possibility that model predictions are inaccurate? This is of paramount significance in industries where mistakes have severe consequences. How confident would a doctor be in a trained model's cancer prognosis if it had a 98 percent accuracy rate? What if, for whatever reason we don't know about, the model fails to detect the majority of malignant cases?
The high accuracy can have been the result of data leakage in the training data, making predictions on new data far less accurate.

## What is machine learning ?

Without getting bogged down in the specifics of Machine Learning's origins within the larger context of AI, it's helpful to review a few key ideas to set the stage for Explainable AI's place in the field and to grasp, from a purely technical perspective, why the need for explainability is so urgent. 
Among the definitions for machine learning, we will focus on this one since it is not only straightforward but also accurate and gets to the heart of the matter:

"Machine learning (ML) is a type of artificial intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning algorithms use historical data as input to predict new output values."

 In the earlier days of computer programming, an algorithm was the go-to method for finding a solution to any issue. The presence of an algorithm assures, all by itself, that the system can be explained and that it is completely transparent. The human mind can understand algorithms, which are defined as processes or sets of rules to be followed in to generate an output given an input.


