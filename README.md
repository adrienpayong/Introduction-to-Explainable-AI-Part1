# Introduction to Explainable AI-Part1
## Introduction

The use of machine learning and AI has increased in recent years. However, continuing adoption is being hampered by some restrictions. One of the major problems with modern machine learning and deep learning algorithms is that their models are not easily interpretable or explainable. As algorithms improve in their predictive abilities, it becomes critical to understand the reasoning behind a given prediction.

A lack of interpretability and explainability in real AI applications makes it hard to trust their predictions. Providing human-friendly explanations would increase trust in machine learning systems and encourage their further use. Emerging as a critical area of study in the next years, explainable AI is now a hot topic in the AI community. This article is an introduction to a series of articles we will publish with an attempt to unbox the black-box model to increase the explainability of the decision made by AI algorithms.

## Mystery of the Black Box
As a result of recent innovations, machine learning algorithms have improved considerably in predictive ability and accuracy. On the other hand, their complexity has grown with time. It's possible to visualize the model's outputs for a given set of inputs without knowing how the model works. Machine learning models can learn the mapping of inputs to outputs by analyzing historical data. This mapping is obvious in certain models, such as decision trees. The prediction process becomes almost impossible for other models, such as random forests and deep learning. The inner workings and subtleties of many machine learning and deep learning models are often hidden.

The confidence we put in and use of these models might suffer as a result of our inability to fully grasp or explain them. How can we evaluate the possibility that model predictions are inaccurate? This is of paramount significance in industries where mistakes have severe consequences. How confident would a doctor be in a trained model's cancer prognosis if it had a 98 percent accuracy rate? What if, for whatever reason we don't know about, the model fails to detect the majority of malignant cases?
The high accuracy can have been the result of data leakage in the training data, making predictions on new data far less accurate.

![source](https://github.com/adrienpayong/object-detection/blob/main/interp.png)

                      [source](https://www.advancinganalytics.co.uk/blog/2021/7/14/shap)

## What is machine learning ?

Without getting bogged down in the specifics of Machine Learning's origins within the larger context of AI, it's helpful to review a few key ideas to set the stage for Explainable AI's place in the field and to grasp, from a purely technical perspective, why the need for explainability is so urgent. 
Among the definitions for machine learning, we will focus on this one since it is not only straightforward but also accurate and gets to the heart of the matter:
"Machine learning (ML) is a type of artificial intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning algorithms use historical data as input to predict new output values."

 In the earlier days of computer programming, an algorithm was the go-to method for finding a solution to any issue. The presence of an algorithm assures, all by itself, that the system can be explained and that it is completely transparent. The human mind can understand algorithms, which are defined as processes or sets of rules to be followed in to generate an output given an input.
 
## Explainable Artificial intelligence: Definition

It's not simple to define Explainable Artificial intelligence (also known as XAI) in a way that takes into account all the possible interpretations. We can define XAI as a collection of approaches and tools that can be used to explain the outcomes generated by ML models in a way that humans can understand.

## An example learning process

- Computer vision is one of the most striking areas where state-of-the-art Deep Learning algorithms have triumphed over more conventional methods. Using tagged image classes, we can teach a convolutional neural network (CNN) to recognize the image.
- We can train a model to recognize and sort out various pneumonia RX images or we can teach it to interpret sign language into spoken languages. The possibilities are endless, but how trustworthy are the results?
- Suppose that our goal is to classify dog and cat images. 
Let's pretend that after training, the algorithm was able to identify between classes with impressive accuracy: Only one misclassification out of 200. But if we apply an Explainable AI technique and ask the model why it has predicted dog, it can provide us with an answer.
- The capacity for a model to articulate its reasoning to people is a great feature. Those well-versed in Machine/Deep Learning can spot flaws in existing models and improve them without much effort. To solve this problem, we can use occlusions (when a portion of an image is obscured) as potential augmentations (when the same image is displayed in several forms).

## The importance of Explainable AI

Classification accuracy alone may not be enough to define and solve our real-world issue; it may not be helpful to know "the what" if we don't also know "the why," or how the model gets at its result. In these circumstances, the prediction is merely a piece of the puzzle. Classification accuracy isn't always excellent, and in most cases, domain-specific experts will remain indispensable in the creation of relevant metrics. 
The need for explainability is most often seen in three key applications that are frequently paired with the prediction of a Machine Learning model:

- **Knowledge Discovery**: This is the most challenging use case to comment on since it involves scenarios in which ML models are used not just to generate predictions but also to deepen one's familiarity with a particular process, event, or system.
- **Model validation**: An explanation is required to determine whether a "Biased" dataset was used to train a Machine Learning model that can discriminate against certain groups of individuals. To properly investigate why an individual was denied a loan, there must be a way to "see inside the black box" and get the decision-making factors that were used for rejection.
- **Model debugging**: If the ML model is to be trusted and resilient, it must provide for debugging, or the ability to see the inner workings of the algorithm that generates results. To make ML systems less vulnerable to assaults intended to fool them, modest changes in the inputs shouldn't result in large changes in the outputs.

## Difference Between Interpretability and Explainability

- To illustrate the difference between interpretability and explainability, consider a pot of water heating slowly over time until it boils, after which the temperature will remain constant.
- The linear trend and the flat temperature beyond the boiling point would be well predicted by ML, but the mechanics of the phase change would remain a mystery.
- Understanding how ML systems predict temperature changes over time in the steady-state regime is an example of interpretability; having an ML model that accounts for the changing state is an example of explainability.
- A Machine Learning model can be considered interpretable if its inner workings can be grasped, even if the reasoning behind its operation remains a mystery.
- Explainability is a theory that works with unseen facts while interpretability focuses on making sense of what is already there and visible.

## Conclusion

Through this article, we have:

- gave the definition of Explainable Artificial intelligence
- provide an example of a Learning Process
- seen the need for explainable AI
- seen the types of explanations
- Settle the difference Between Interpretability and Explainability
- Seen the techniques to make a ML model explainable

